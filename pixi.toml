[workspace]
authors = ["boilcy <0x6c6379@gmail.com>"]
channels = ["conda-forge", "pytorch", "nvidia", "cuda"]
name = "qarobo"
platforms = ["win-64", "linux-64", "linux-aarch64"]
version = "0.1.0"

[system-requirements]
cuda = "12.0"

[target.unix.activation.env]
CUDA_HOME="$CONDA_PREFIX"
LD_LIBRARY_PATH="$CONDA_PREFIX/lib:$LD_LIBRARY_PATH"
[target.win-64.activation.env]
CUDA_HOME="%CONDA_PREFIX%"

# --- Common ---
[dependencies]
cuda-version="12.6.*"
loguru = ">=0.7.3,<0.8"
numpy = ">=2.2.6,<3"
requests = ">=2.32.5,<3"
transformers = ">=4.56.1,<5"
nltk = ">=3.9.1,<4"
openai = ">=1.107.1,<2"
setuptools = ">=75.8.2,<81"
librosa = ">=0.11.0,<0.12"
pip = ">=25.2,<26"
portaudio = ">=19.7.0,<20"

[pypi-dependencies]
misaki = { version = "*", extras = ["zh", "en"] }
kokoro = ">=0.9.4, <0.10"
sounddevice = ">=0.5.2, <0.6"

[feature.py310.dependencies]
python = "3.10.*"

[feature.py312.dependencies]
python = "3.12.*"

# --- Feature for Desktop GPU (e.g., 4090) ---
[feature.torch-cuda.dependencies]
pytorch-gpu = ">=2.5.1,<3"
torchaudio = ">=2.5.1,<3"

[feature.torch-cuda.target.linux.dependencies]
triton = ">=3.3.1,<4"

# --- Feature for Jetson Orin Nano (JP6) ---
[feature.torch-jetson-jp6]
platforms = ["linux-aarch64"]

[feature.torch-jetson-jp6.pypi-dependencies]
torch = { version = ">=2.5.1", index = "https://pypi.jetson-ai-lab.io/jp6/cu126" }
torchvision = { version = ">=0.20.1", index = "https://pypi.jetson-ai-lab.io/jp6/cu126" }
torchaudio = { version = ">=2.5.1", index = "https://pypi.jetson-ai-lab.io/jp6/cu126" }
triton = { version = ">=3.4.0", index = "https://pypi.jetson-ai-lab.io/jp6/cu126" }

# --- Feature for Jetson Thor (SBSA) ---
[feature.torch-jetson-sbsa]
platforms = ["linux-aarch64"]

[feature.torch-jetson-sbsa.pypi-dependencies]
torch = { version = ">=2.9.0", index = "https://pypi.jetson-ai-lab.io/sbsa/cu130" }
torchvision = { version = ">=0.24.0", index = "https://pypi.jetson-ai-lab.io/sbsa/cu130" }
torchaudio = { version = ">=2.9.0", index = "https://pypi.jetson-ai-lab.io/sbsa/cu130" }
triton = { version = ">=3.5.0", index = "https://pypi.jetson-ai-lab.io/sbsa/cu130" }

[pypi-options]
index-url = "https://pypi.tuna.tsinghua.edu.cn/simple"
extra-index-urls = ["https://pypi.org/simple", "https://mirrors.aliyun.com/pypi/simple/", "https://mirrors.hust.edu.cn/pypi/web/simple"]
no-build-isolation = true

[environments]
desktop = { features = ["py312", "torch-cuda"], solve-group = "desktop" }
orin-nano = { features = ["py310", "torch-jetson-jp6"], solve-group = "jetson-jp6" }
thor = { features = ["py312", "torch-jetson-sbsa"], solve-group = "jetson-sbsa" }

[tasks.server]
cmd = [
    "python",
    "s2s_pipeline.py",
    "--lm_model_name",
    "./models/Qwen2.5-3B-Instruct",
    "--stt_model_name",
    "./models/whisper-large-v3-turbo",
    "--recv_host", 
    "0.0.0.0",
    "--send_host",
    "0.0.0.0",
    "--language",
    "zh",
    "--tts",
    "kokoro",
    "--init_chat_role",
    "system",
    "--init_chat_prompt",
    "你是一个帮助用户解决问题的助手"
]

[tasks.client]
args = [{ "arg" = "host", "default" = "127.0.0.1" }]
cmd = [
    "python",
    "listen_and_play.py",
    "--host",
    "{{host}}"
]

[tasks.local]
args = [
  { "arg" = "log_level", "default" = "INFO" }
]
cmd = [
    "python",
    "s2s_pipeline.py",
    # vad setup
    "--vad_model_name",
    "G:/models/silero-vad",
    # stt setup
    "--stt_model_name",
    "G:/models/whisper-large-v3-turbo",
    # llm setup
    "--lm_model_name",
    "G:/models/Qwen3-0.6B",
    "--lm_gen_max_new_tokens",
    "256",
    "--lm_gen_temperature",
    "0.7",
    "--lm_gen_top_p",
    "0.8",
    "--lm_gen_top_k",
    "20",
    "--lm_gen_do_sample",
    "True",
    "--init_chat_role",
    "system",
    "--init_chat_prompt",
    "You are a helpful and friendly AI assistant. You are polite, respectful, and aim to provide concise responses of less than 20 words.",
    "--chat-size",
    "16",
    # tts setup
    "--tts",
    "kokoro",
    "--tts_model_name",
    "G:/models/Kokoro-82M-v1.1-zh",
    "--tts_default_voice",
    "zf_088",
    "--mode",
    "local",
    "--language",
    "zh",
    "--log_level",
    "{{log_level}}"
]

[tasks.format]
cmd = "pixi exec --spec ruff ruff format ."